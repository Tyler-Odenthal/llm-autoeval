{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Tyler-Odenthal/llm-autoeval/blob/master/LLM_AutoEval.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title # üßê LLM AutoEval\n",
        "\n",
        "# @markdown > üó£Ô∏è [Large Language Model Course](https://github.com/mlabonne/llm-course)\n",
        "\n",
        "# @markdown This notebook allows you to **automatically evaluate your LLMs** using RunPod. If you don't have an account, please consider using my [referral link](https://runpod.io?ref=9nvk2srl).\n",
        "\n",
        "# @markdown Once a pod has started, you can safely close this tab. The results are then privately uploaded to [GitHub Gist](https://gist.github.com/), and the pod is automatically destroyed.\n",
        "\n",
        "# @markdown For further details, see the project on üíª [GitHub](https://github.com/mlabonne/llm-autoeval).\n",
        "\n",
        "!pip install -qqq runpod --progress-bar off\n",
        "\n",
        "import runpod\n",
        "from google.colab import userdata\n",
        "\n",
        "# @markdown ---\n",
        "\n",
        "# @markdown ## Evaluation parameters\n",
        "BENCHMARK = \"openllm\" # @param [\"nous\", \"openllm\", \"peft-nous\"]\n",
        "MODEL_ID = \"todenthal/mistral-patent-merge-test\" # @param {type:\"string\"}\n",
        "GPU = \"NVIDIA A100 80GB PCIe\" # @param [\"NVIDIA A100 80GB PCIe\", \"NVIDIA A100-SXM4-80GB\", \"NVIDIA A30\", \"NVIDIA A40\", \"NVIDIA GeForce RTX 3070\", \"NVIDIA GeForce RTX 3080\", \"NVIDIA GeForce RTX 3080 Ti\", \"NVIDIA GeForce RTX 3090\", \"NVIDIA GeForce RTX 3090 Ti\", \"NVIDIA GeForce RTX 4070 Ti\", \"NVIDIA GeForce RTX 4080\", \"NVIDIA GeForce RTX 4090\", \"NVIDIA H100 80GB HBM3\", \"NVIDIA H100 PCIe\", \"NVIDIA L4\", \"NVIDIA L40\", \"NVIDIA RTX 4000 Ada Generation\", \"NVIDIA RTX 4000 SFF Ada Generation\", \"NVIDIA RTX 5000 Ada Generation\", \"NVIDIA RTX 6000 Ada Generation\", \"NVIDIA RTX A2000\", \"NVIDIA RTX A4000\", \"NVIDIA RTX A4500\", \"NVIDIA RTX A5000\", \"NVIDIA RTX A6000\", \"Tesla V100-FHHL-16GB\", \"Tesla V100-PCIE-16GB\", \"Tesla V100-SXM2-16GB\", \"Tesla V100-SXM2-32GB\"]\n",
        "NUMBER_OF_GPUS = 4 # @param {type:\"slider\", min:1, max:8, step:1}\n",
        "CONTAINER_DISK = 100 # @param {type:\"slider\", min:50, max:1000, step:25}\n",
        "CLOUD_TYPE = \"SECURE\" # @param [\"COMMUNITY\", \"SECURE\"]\n",
        "REPO = \"https://github.com/Tyler-Odenthal/llm-autoeval.git\" # @param {type:\"string\"}\n",
        "TRUST_REMOTE_CODE = True # @param {type:\"boolean\"}\n",
        "DEBUG = False # @param {type:\"boolean\"}\n",
        "\n",
        "# @markdown ---\n",
        "\n",
        "# @markdown ## Tokens\n",
        "# @markdown Enter the name of your tokens in the Secrets tab.\n",
        "RUNPOD_TOKEN = \"S2JHIFMRYQLGBJS1C1ICP97L95KOZWD90QXIKQXE\" # @param {type:\"string\"}\n",
        "GITHUB_TOKEN = \"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\" # @param {type:\"string\"}\n",
        "\n",
        "# Environment variables\n",
        "runpod.api_key = \"S2JHIFMRYQLGBJS1C1ICP97L95KOZWD90QXIKQXE\" #userdata.get('RUNPOD_API')\n",
        "GITHUB_API_TOKEN = \"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\" #userdata.get('GITHUB_API')\n",
        "\n",
        "# Create a pod\n",
        "pod = runpod.create_pod(\n",
        "    name=f\"Eval {MODEL_ID.split('/')[-1]} on {BENCHMARK.capitalize()}\",\n",
        "    image_name=\"runpod/pytorch:2.0.1-py3.10-cuda11.8.0-devel-ubuntu22.04\",\n",
        "    gpu_type_id=GPU,\n",
        "    cloud_type=CLOUD_TYPE,\n",
        "    gpu_count=NUMBER_OF_GPUS,\n",
        "    volume_in_gb=0,\n",
        "    container_disk_in_gb=CONTAINER_DISK,\n",
        "    template_id=\"au6nz6emhk\",\n",
        "    env={\n",
        "        \"BENCHMARK\": BENCHMARK,\n",
        "        \"MODEL_ID\": MODEL_ID,\n",
        "        \"REPO\": REPO,\n",
        "        \"TRUST_REMOTE_CODE\": TRUST_REMOTE_CODE,\n",
        "        \"DEBUG\": DEBUG,\n",
        "        \"GITHUB_API_TOKEN\": GITHUB_API_TOKEN,\n",
        "        \"NUMBER_OF_GPUS\": NUMBER_OF_GPUS,\n",
        "    }\n",
        ")\n",
        "\n",
        "print(\"Pod started: https://www.runpod.io/console/pods\")"
      ],
      "metadata": {
        "id": "elyxjYI_rY5W",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "30a1fa0f-7fd7-4837-ad81-2616c2be6c9c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pod started: https://www.runpod.io/console/pods\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pod"
      ],
      "metadata": {
        "id": "h8RQ8fF9sXWj",
        "outputId": "a75bdeed-0b28-49d9-e734-dec4df482327",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'id': 'u6e8jbf5ncoeq0',\n",
              " 'desiredStatus': 'RUNNING',\n",
              " 'imageName': 'runpod/pytorch:2.0.1-py3.10-cuda11.8.0-devel-ubuntu22.04',\n",
              " 'env': ['BENCHMARK=openllm',\n",
              "  'MODEL_ID=arcee-ai/Mistral-Lora-Adapter-CS-Slerp',\n",
              "  'REPO=https://github.com/mmcquade11/llm-autoeval.git',\n",
              "  'TRUST_REMOTE_CODE=True',\n",
              "  'DEBUG=True',\n",
              "  'GITHUB_API_TOKEN=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',\n",
              "  'PUBLIC_KEY=ssh-ed25519 AAAAC3NzaC1lZDI1NTE5AAAAILpypfsW2z9+yaNY5SsIQVNA2xqdcX1yLloSF4wh+2Ll markmcquade@Marks-MacBook-Air.local\\nssh-ed25519 AAAAC3NzaC1lZDI1NTE5AAAAIK36cbYSl4v0B8j/kUUvYN0AL+n89oD3K5ERBKaXr5ZG tleyden@Tleyden-MacBook-Pro-M2-Max.local\\nssh-ed25519 AAAAC3NzaC1lZDI1NTE5AAAAIAZWyZXvLEa5BU0JLZYCW5MESE9wN/C3T/p9A3XpilIx charles@arcee.ai'],\n",
              " 'machineId': '15bzjeq4khht',\n",
              " 'machine': {'podHostId': 'u6e8jbf5ncoeq0-64410f75'}}"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -qqq runpod --progress-bar off"
      ],
      "metadata": {
        "id": "_CWRiMdakqmr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!curl -X POST https://api.runpod.ai/v2/stable-diffusion-v1/run \\\n",
        "-H 'Content-Type: application/json'                             \\\n",
        "-H 'Authorization: Bearer S2JHIFMRYQLGBJS1C1ICP97L95KOZWD90QXIKQXE'    \\\n",
        "-d '{\"input\": {\"prompt\": \"a cute magical flying dog, fantasy art drawn by disney concept artists\"}}'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rYN78hj3nBom",
        "outputId": "a038c108-c56b-49f2-8393-bda28f6e83f4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"id\":\"f165f7e2-ac3a-4d49-af0c-16788c58d6e1-u1\",\"status\":\"IN_QUEUE\"}"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "J45JbM40nsON"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}